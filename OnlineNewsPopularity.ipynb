{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OnlineNewsPopularity.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXp+O572AInvwDo/uJMDg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsriniva/DS-Unit-2-Build/blob/main/OnlineNewsPopularity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "ui30Tz-ZI2le",
        "outputId": "c9800874-5d94-4713-b62e-b7daf78a60a3"
      },
      "source": [
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/nsriniva/DS-Unit-2-Build/master/'\n",
        "    '''\n",
        "    !pip install category_encoders==2.*\n",
        "    !pip install eli5\n",
        "    !pip install pdpbox\n",
        "    !pip install shap\n",
        "    '''\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'\n",
        "\n",
        "from collections import OrderedDict\n",
        "from math import isclose\n",
        "import zipfile \n",
        "from urllib.request import urlopen\n",
        "import io\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, \\\n",
        "                                 LogisticRegressionCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "try:\n",
        "  from category_encoders import OrdinalEncoder, OneHotEncoder\n",
        "except:\n",
        "  !pip install category_encoders==2.*\n",
        "  from category_encoders import OrdinalEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "try:\n",
        "  import eli5\n",
        "except:\n",
        "  !pip install eli5\n",
        "  import eli5\n",
        "\n",
        "from eli5.sklearn import PermutationImportance\n",
        "try:\n",
        "  import shap\n",
        "except:\n",
        "  !pip install shap\n",
        "  import shap\n",
        "\n",
        "try:\n",
        "  from pdpbox.pdp import pdp_isolate, pdp_plot, pdp_interact, pdp_interact_plot\n",
        "except:\n",
        "  !pip install pdpbox\n",
        "  from pdpbox.pdp import pdp_isolate, pdp_plot, pdp_interact, pdp_interact_plot\n",
        "\n",
        "\n",
        "# For details about the data cleanup, please see \n",
        "# https://github.com/nsriniva/DS-Unit-2-Applied-Modeling/blob/master/CleanupOnlineNewsPopularity.ipynb\n",
        "# and 'The Dataset' section of\n",
        "# https://nsriniva.github.io/2020-10-23-DSPT9-Unit1-BuildProject/\n",
        "\n",
        "# Cleaned up and uploaded csv data file from \n",
        "# https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip \n",
        "# in\n",
        "# https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity\n",
        "# to my github repo as\n",
        "# https://github.com/nsriniva/DS-Unit-2-Applied-Modeling/blob/master/OnlineNewsPopularity.csv.zip?raw=true\n",
        "\n",
        "# The associated names file is available at\n",
        "# https://raw.githubusercontent.com/nsriniva/DS-Unit-2-Applied-Modeling/master/OnlineNewsPopularity.names\n",
        "\n",
        "\n",
        "onp_url = DATA_PATH + 'OnlineNewsPopularity.csv.zip'\n",
        "\n",
        "onp_df = pd.read_csv(onp_url, compression='zip')\n",
        "\n",
        "null_values = onp_df.isna().sum().sum()\n",
        "\n",
        "print(f\"There are {['','no'][int(null_values==0)]} invalid values in the dataset!\")\n",
        "\n",
        "# The zscore() method from the scipy.stats package is used to compute z scores \n",
        "# for the shares values. These z scores is compared against the specified  \n",
        "# sigma value to generate a boolean filter array that could be used to \n",
        "# paritition the dataset based on whether the zscore is greater than the\n",
        "# specified sigma.\n",
        "def get_sigma_filter(df, sigma=0.5):\n",
        "  z = np.abs(stats.zscore(df.shares))\n",
        "  return np.where(z>sigma)[0]\n",
        "\n",
        "# Use the boolean filter array provided by get_sigma_filter() to\n",
        "# ignore entries with zscore greater than 0.5 and compute the\n",
        "# median and max 'shares' values for the remaining entries.\n",
        "def classification_marks(df):\n",
        "\n",
        "  shares_info = df.drop(get_sigma_filter(df)).shares\n",
        "\n",
        "  max = shares_info.max()\n",
        "  median = shares_info.median()\n",
        "\n",
        "  return median, max\n",
        "\n",
        "\n",
        "shares_median = onp_df.shares.median()\n",
        "\n",
        "print(shares_median)\n",
        "# Use the medium(median) value to classify articles into \n",
        "# unpopular(0) and popular(1)  \n",
        "onp_df['popularity'] = onp_df.shares.apply(lambda x: 0 if x < shares_median else 1)\n",
        "\n",
        "display(onp_df.shape)\n",
        "\n",
        "# Remove outliers\n",
        "def remove_outliers(df, sigma=0.5):\n",
        "  df = df.copy()\n",
        "  return df.drop(get_sigma_filter(df, sigma))\n",
        "\n",
        "\n",
        "onp_no_df = onp_df.copy()\n",
        "\n",
        "#onp_no_df = remove_outliers(onp_no_df, 0.25)\n",
        "\n",
        "shares_median = onp_no_df.shares.median()\n",
        "\n",
        "print(shares_median)\n",
        "\n",
        "# Use the medium(median) value to classify articles into \n",
        "# unpopular(0) and popular(1)  \n",
        "onp_no_df['popularity'] = onp_no_df.shares.apply(lambda x: 0 if x < shares_median else 1)\n",
        "\n",
        "display(onp_no_df.shape)\n",
        "\n",
        "\n",
        "\n",
        "# The baseline accuracy or the value we'd get by just guessing that that the\n",
        "# value is always the majority class\n",
        "\n",
        "target = 'popularity'\n",
        "\n",
        "baseline_accuracy = onp_no_df[target].value_counts(normalize=True).max()\n",
        "\n",
        "print(f'baseline_accuracy = {baseline_accuracy:0.4f}')\n",
        "\n",
        "# Drop the 'shares' column used to derive 'popularity' along\n",
        "# with the non predictive 'url' and 'timedelta' columns.\n",
        "drop_cols = ['shares', 'url', 'timedelta']\n",
        "\n",
        "onp_no_df = onp_no_df.drop(columns=drop_cols)\n",
        "# Will use a random split of 64% Training, 16% Validation and 20% Test \n",
        "\n",
        "X = onp_no_df.drop(columns=target)\n",
        "y = onp_no_df[target]\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X,y,train_size=0.8, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, train_size=0.8, random_state=42)\n",
        "\n",
        "display(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)\n",
        "\n",
        "display(y_train.value_counts(normalize=True))\n",
        "baseline_accuracy = y_train.value_counts(normalize=True).max()\n",
        "\n",
        "print(f'baseline_accuracy = {baseline_accuracy:0.4f}')\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "There are no invalid values in the dataset!\n",
            "1400.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(39644, 51)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1400.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(39644, 51)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "baseline_accuracy = 0.5336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(25372, 47)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(6343, 47)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(7929, 47)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(25372,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(6343,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(7929,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1    0.536891\n",
              "0    0.463109\n",
              "Name: popularity, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "baseline_accuracy = 0.5369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M9c08b2MiS8"
      },
      "source": [
        "def model_acc_auc(model, *Xyn, names=None):\n",
        "\n",
        "  if names is None:\n",
        "    names = ['Training', 'Validation', 'Test']\n",
        "  p = Xyn\n",
        "  n = len(p)\n",
        "\n",
        "  assert n >= 2\n",
        "  assert n%2 == 0\n",
        "\n",
        "  n = n//2\n",
        "  Xn = []\n",
        "  yn = []\n",
        "  for i in range(0,n):\n",
        "    Xn.append(p[2*i])\n",
        "    yn.append(p[2*i+1])\n",
        "\n",
        "  accn = []\n",
        "  aucn = []\n",
        "  bl = []\n",
        "  for i,X in enumerate(Xn):\n",
        "    bl.append(yn[i].value_counts(normalize=True).max())\n",
        "\n",
        "    accn.append(model.score(X, yn[i]))\n",
        "    \n",
        "    y_proba = model.predict_proba(X)[:,-1]\n",
        "    aucn.append(roc_auc_score(yn[i], y_proba))\n",
        "  \n",
        "  for i, acc in enumerate(zip(accn, bl, aucn)):\n",
        "    name = names[i]\n",
        "    print(f'{name} Accuracy : {acc[0]:0.4f}/{acc[1]:0.4f}/{acc[2]:0.4f}')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVFTrx2UIjXd"
      },
      "source": [
        "# For parameter k, use SelectKBest to compute the k best\n",
        "# features and use those to train a LogisticRegressionCV\n",
        "# model.\n",
        "def select_and_fit(k, X_tr, y_tr, X_v, y_v):\n",
        "    \n",
        "  selector = SelectKBest(score_func=f_classif, k=k)\n",
        "  X_train_selected = selector.fit_transform(X_tr, y_train)\n",
        "  X_val_selected = selector.transform(X_v)\n",
        "\n",
        "  model = LogisticRegressionCV()\n",
        "  model.fit(X_train_selected, y_tr)\n",
        "  \n",
        "  return model.score(X_val_selected, y_v), model, selector\n",
        "\n",
        "def get_best_k_model(X_tr, y_tr, X_v, y_v):\n",
        "  best_model = None\n",
        "  best_selector = None\n",
        "  best_features=[]\n",
        "  best_k = 0\n",
        "  best_acc = 0\n",
        "\n",
        "  # n = 62\n",
        "  n = X_tr.shape[1]\n",
        "  # Loop through k and compare accuracies to determine the best\n",
        "  # k features(best_features) with the highest accuracy\n",
        "  # One run with k from 1 - 62(range(1,n+1)) gave the best k as 51 - in order to reduce\n",
        "  # the time looking for best k, we just run once with k=51\n",
        "  for k in range(51, 52):\n",
        "      acc, model, selector = select_and_fit(k, X_tr, y_tr, X_v, y_v)\n",
        "      #print(acc, feat)\n",
        "      if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_k = k\n",
        "        best_model = model\n",
        "        best_selector = selector\n",
        "\n",
        "  print(f'best_k = {best_k}\\nbest Accuracy = {best_acc:0.4f}\\n')\n",
        "\n",
        "  return best_acc, best_k, best_selector, best_model\n",
        "\n",
        "encoder = OneHotEncoder(use_cat_names=True)\n",
        "\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_val_encoded = encoder.transform(X_val)\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_transformed = scaler.fit_transform(X_train_encoded)\n",
        "X_val_transformed = scaler.transform(X_val_encoded)\n",
        "X_test_transformed = scaler.transform(X_test_encoded)\n",
        "\n",
        "best_acc, best_k, best_selector, best_model = get_best_k_model(X_train_transformed, y_train, X_val_transformed, y_val)\n",
        "\n",
        "model_acc_auc(best_model, best_selector.transform(X_train_transformed), y_train, best_selector.transform(X_val_transformed), y_val, best_selector.transform(X_test_transformed), y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwvZObO8en0V"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,30))\n",
        "\n",
        "\n",
        "best_features = X_train_encoded.columns[best_selector.get_support()]\n",
        "# Get and plot model coefficients.\n",
        "coefficients = pd.Series(best_model.coef_[0], best_features)\n",
        "coefficients.sort_values().plot.barh(); #bar charts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgCMRbjGLEjI"
      },
      "source": [
        "# Simple model, with OrdinalEncoder for the data_channel and weekday categorical\n",
        "# columns and a DecisionTreeClassifier with default parameter values.\n",
        "model = make_pipeline(\n",
        "  OrdinalEncoder(), \n",
        "  DecisionTreeClassifier()\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "display(y_train.value_counts(normalize=True))\n",
        "display(y_val.value_counts(normalize=True))\n",
        "\n",
        "training_bl = y_train.value_counts(normalize=True).max()\n",
        "validation_bl = y_val.value_counts(normalize=True).max()\n",
        "test_bl = y_test.value_counts(normalize=True).max()\n",
        " \n",
        "model_acc_auc(model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "transformers = make_pipeline(\n",
        "    OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='median')\n",
        ")\n",
        "\n",
        "X_train_transformed = transformers.fit_transform(X_train)\n",
        "X_val_transformed = transformers.transform(X_val)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=103, random_state=42, n_jobs=-1, max_depth=25, min_samples_leaf=3, max_features=0.3)\n",
        "model.fit(X_train_transformed, y_train)\n",
        "\n",
        "model_acc_auc(model, X_train_transformed, y_train, X_val_transformed, y_val)\n",
        "\n",
        "permuter = PermutationImportance(\n",
        "    model, \n",
        "    scoring='accuracy', \n",
        "    n_iter=5, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "permuter.fit(X_val_transformed, y_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QrSkIF0MCNa"
      },
      "source": [
        "feature_names = X_val.columns.tolist()\n",
        "\n",
        "eli5.show_weights(\n",
        "    permuter, \n",
        "    top=None, # No limit: show permutation importances for all features\n",
        "    feature_names=feature_names # must be a list\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMgTd4lWMG3W"
      },
      "source": [
        "'''\n",
        "print('Shape before removing', X_train.shape)\n",
        "\n",
        "minimum_importance = 0 \n",
        "mask = permuter.feature_importances_ > minimum_importance\n",
        "features = X_train.columns[mask]\n",
        "X_train = X_train[features]\n",
        "\n",
        "print('Shape after removing ', X_train.shape)\n",
        "\n",
        "X_val = X_val[features]\n",
        "X_test = X_test[features]\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMwx8GsbMOf3"
      },
      "source": [
        "model = make_pipeline(\n",
        "  OrdinalEncoder(), \n",
        "  DecisionTreeClassifier(max_depth=7,random_state=42, min_samples_leaf=3)\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "display(y_train.value_counts(normalize=True))\n",
        "display(y_val.value_counts(normalize=True))\n",
        "\n",
        "model_acc_auc(model, X_train, y_train, X_val, y_val)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5qSPBjmMXVM"
      },
      "source": [
        "pipe = make_pipeline(\n",
        "    OrdinalEncoder(), \n",
        "    RandomForestClassifier(n_estimators=103, random_state=42, n_jobs=-1, max_depth=25, min_samples_leaf=3, max_features=0.3)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "model_acc_auc(pipe, X_train, y_train, X_val, y_val, X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m1XFQ3vMsRC"
      },
      "source": [
        "encoder = OrdinalEncoder()\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_val_encoded = encoder.transform(X_val)\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "\n",
        "eval_set = [(X_train_encoded, y_train), \n",
        "            (X_val_encoded, y_val)]\n",
        "\n",
        "model = XGBClassifier(n_estimators=1000, random_state=42, n_jobs=-1, max_depth=6, learning_rate=0.5) \n",
        "\n",
        "eval_metric = 'auc'\n",
        "model.fit(X_train_encoded, y_train, \n",
        "          eval_set=eval_set, \n",
        "          eval_metric=eval_metric,\n",
        "          early_stopping_rounds=400)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tNGuNxbOInn"
      },
      "source": [
        "results = model.evals_result()\n",
        "train_error = results['validation_0'][eval_metric]\n",
        "val_error = results['validation_1'][eval_metric]\n",
        "epoch = list(range(1, len(train_error)+1))\n",
        "plt.plot(epoch, train_error, label='Train')\n",
        "plt.plot(epoch, val_error, label='Validation')\n",
        "plt.ylabel(f'Classification {eval_metric.capitalize()}')\n",
        "plt.xlabel('Model Complexity (n_estimators)')\n",
        "plt.title('Validation Curve for this XGBoost model')\n",
        "#plt.ylim((0.18, 0.22)) # Zoom in\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq_512CFQEVL"
      },
      "source": [
        "model_acc_auc(model, X_train_encoded, y_train, X_val_encoded, y_val, X_test_encoded, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJEVEn37OgVV"
      },
      "source": [
        "features = ['is_weekend', 'kw_avg_avg']\n",
        "\n",
        "print(X_train_encoded.shape, X_val_encoded.shape)\n",
        "\n",
        "isolated = []\n",
        "\n",
        "for feature in features:\n",
        "  isolated.append(\n",
        "      pdp_isolate(\n",
        "      model=model, \n",
        "      dataset=X_val_encoded, \n",
        "      model_features=X_val_encoded.columns, \n",
        "      feature=feature\n",
        "    )\n",
        "  )\n",
        "\n",
        "interaction = pdp_interact(\n",
        "    model=model, \n",
        "    dataset=X_val_encoded, \n",
        "    model_features=X_val_encoded.columns,\n",
        "    features=features\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl9Wk8N1PU6a"
      },
      "source": [
        "for idx,elem in enumerate(isolated):\n",
        "  pdp_plot(elem, feature_name=features[idx]);\n",
        "  if features[idx] == 'is_weekend':\n",
        "    # Manually change the xticks labels\n",
        "    plt.xticks([0, 1], ['False', 'True']);\n",
        "\n",
        "pdp = interaction.pdp.pivot_table(\n",
        "    values='preds', \n",
        "    columns=features[0], # First feature on x axis\n",
        "    index=features[1]    # Next feature on y axis\n",
        ")[::-1]  # Reverse the index order so y axis is ascending\n",
        "\n",
        "pdp = pdp.rename(columns={0:'False', 1:'True'})\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(pdp, annot=True, fmt='.3f', cmap='viridis')\n",
        "plt.title('Partial Dependence of Article Popularity on is_weekend & kw_avg_avg');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTCTC4clPcVu"
      },
      "source": [
        "\n",
        "df = pd.DataFrame({\n",
        "    'pred_proba': model.predict_proba(X_test_encoded)[:, -1] , \n",
        "    'popular': y_test\n",
        "}) \n",
        "\n",
        "df= df.merge(X_test_encoded, left_index=True, right_index=True)\n",
        "\n",
        "popular = df.popular == 1\n",
        "unpopular = df.popular == 0\n",
        "popular_right = (popular) == (df['pred_proba'] > 0.50)\n",
        "unpopular_right = (unpopular) == (df['pred_proba'] <= 0.50)\n",
        "\n",
        "\n",
        "popular_correct = df[popular&popular_right]\n",
        "unpopular_correct = df[unpopular&unpopular_right]\n",
        "\n",
        "correct = df[popular&popular_right | unpopular&unpopular_right]\n",
        "\n",
        "display(popular_correct.shape, unpopular_correct.shape, correct.shape, df.shape)\n",
        "display(correct.sample(n=10, random_state=1).sort_values(by='pred_proba'))\n",
        "\n",
        "row_popular = X_test_encoded.loc[[25641]]\n",
        "\n",
        "row_unpopular = X_test_encoded.loc[[5864]]\n",
        "\n",
        "display(row_popular, row_unpopular)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47txRwviUAuu"
      },
      "source": [
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "shap.initjs()\n",
        "\n",
        "shap_values = explainer.shap_values(row_popular)\n",
        "shap.force_plot(\n",
        "    base_value=explainer.expected_value, \n",
        "    shap_values=shap_values, \n",
        "    features=row_popular,\n",
        "    #matplotlib=True, # This does not work if link is set to 'logit'\n",
        "    link='logit' # For classification, this shows predicted probabilities\n",
        ")\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdj4JFgHUMjp"
      },
      "source": [
        "shap.initjs()\n",
        "\n",
        "shap_values = explainer.shap_values(row_unpopular)\n",
        "shap.force_plot(\n",
        "    base_value=explainer.expected_value, \n",
        "    shap_values=shap_values, \n",
        "    features=row_unpopular,\n",
        "    #matplotlib=True, # This does not work if link is set to 'logit'\n",
        "    link='logit' # For classification, this shows predicted probabilities\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}